{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a1b30-dc97-421f-9fa1-3d2b22a7dfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "      Author  : Suresh Pokharel\n",
    "      Email   : sureshp@mtu.edu\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import required libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "import re\n",
    "import gc\n",
    "import esm\n",
    "import ankh\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"\n",
    "define file paths and other parameters\n",
    "\"\"\"\n",
    "input_fasta_file = \"input/sequence.fasta\" # load test sequence\n",
    "output_csv_file = \"output/results.csv\" \n",
    "prot_t5_model_path = 'models/protT5_model_ann.h5'\n",
    "esm2_model_path = 'models/esm3B_model_ann.h5'\n",
    "ankh_model_path = 'models/ankh_model_ann.h5'\n",
    "\n",
    "cutoff_threshold_ankh = 0.496\n",
    "cutoff_threshold_esm = 0.797\n",
    "cutoff_threshold_prott5 = 0.669\n",
    "\n",
    "\n",
    "# create results dataframe\n",
    "results_df = pd.DataFrame(columns = ['prot_desc', 'position','site_residue', 'ankh_prob(Th = 0.496)','prot_t5_prob(Th = 0.669)','esm2_prob(Th = 0.797)', 'final_prediction'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load tokenizer and pretrained model ProtT5, Ankh, ESM\n",
    "\"\"\"\n",
    "# install SentencePiece transformers if not installed already\n",
    "#!pip install -q SentencePiece transformers\n",
    "\n",
    "tokenizer_prot_t5 = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False )\n",
    "pretrained_model_prot_t5 = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "\n",
    "# pretrained_model = pretrained_model.half()\n",
    "gc.collect()\n",
    "\n",
    "# define devices\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "pretrained_model_prot_t5 = pretrained_model_prot_t5.to(device)\n",
    "pretrained_model_prot_t5 = pretrained_model_prot_t5.eval()\n",
    "\n",
    "\n",
    "# To load ankh model:\n",
    "ankh_pretrained_model, tokenizer_ankh = ankh.load_large_model()\n",
    "ankh_pretrained_model.eval()\n",
    "\n",
    "\n",
    "# Load ESM-2 model\n",
    "pretrained_model_esm, alphabet_esm = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "batch_converter_esm = alphabet_esm.get_batch_converter()\n",
    "pretrained_model_esm.eval()  # disables dropout for deterministic results\n",
    "\n",
    "\n",
    "\n",
    "def get_1021_string(sequence, site=0):\n",
    "    \n",
    "    \"\"\" \n",
    "        We are taking one sequence at a time because of the memory issue, this can be improved a lot\n",
    "    \"\"\"\n",
    "    \n",
    "    # truncate sequence to peptide of 1024 if it is greater\n",
    "    if len(sequence) > 1021:\n",
    "        if site < 511:\n",
    "            # take first 1023 window\n",
    "            sequence_truncated = sequence[:1021]\n",
    "            new_site = site\n",
    "            \n",
    "        elif site > len(sequence)-511:\n",
    "            sequence_truncated = sequence[-1021:]\n",
    "            new_site = 1021 - (len(sequence) - site) \n",
    "        else:\n",
    "            # Use new position just to extract the feature, store original \n",
    "            sequence_truncated = sequence[site - 510 : site + 510 + 1]\n",
    "            new_site = 510\n",
    "    else: \n",
    "        # change nothing\n",
    "        new_site = site\n",
    "        sequence_truncated = sequence\n",
    "        \n",
    "    return new_site, sequence_truncated\n",
    "\n",
    "def get_ankh_features(sequence):\n",
    "\n",
    "    def embed_dataset(model, sequences, shift_left = 0, shift_right = -1):\n",
    "        inputs_embedding = []\n",
    "        with torch.no_grad():\n",
    "            ids = tokenizer_ankh.batch_encode_plus([[sequence]], \n",
    "                                              add_special_tokens=True, \n",
    "                                              padding=True,\n",
    "                                              return_tensors=\"pt\",\n",
    "                                              is_split_into_words=True\n",
    "                                             )\n",
    "            embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
    "            embedding = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n",
    "        return embedding\n",
    "    \n",
    "    def preprocess_dataset(sequences, max_length=None):\n",
    "        '''\n",
    "            Args:\n",
    "                sequences: list, the list which contains the protein primary sequences.\n",
    "                labels: list, the list which contains the dataset labels.\n",
    "                max_length, Integer, the maximum sequence length, \n",
    "                if there is a sequence that is larger than the specified sequence length will be post-truncated. \n",
    "        '''\n",
    "        if max_length is None:\n",
    "            max_length = len(max(sequences, key=lambda x: len(x)))\n",
    "        splitted_sequences = [list(seq[:max_length]) for seq in sequences]\n",
    "        return splitted_sequences\n",
    "\n",
    "    training_embeddings = embed_dataset(ankh_pretrained_model, preprocess_dataset([sequence]))  \n",
    "    \n",
    "    return training_embeddings\n",
    "    \n",
    "    \n",
    "def get_protT5_features(sequence): \n",
    "    \n",
    "    \"\"\"\n",
    "    Description: Extract a window from the given string at given position of given size\n",
    "                (Need to test more conditions, optimizations)\n",
    "    Input:\n",
    "        sequence (str): str of length l\n",
    "    Returns:\n",
    "        tensor: l*1024\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace rare amino acids with X\n",
    "    sequence = re.sub(r\"[UZOB]\", \"X\", sequence)\n",
    "    \n",
    "    # add space in between amino acids\n",
    "    sequence = [ ' '.join(sequence)]\n",
    "    \n",
    "    # set configurations and extract features\n",
    "    ids = tokenizer_prot_t5.batch_encode_plus(sequence, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = pretrained_model_prot_t5(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "    \n",
    "    # find length\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    \n",
    "    # select features\n",
    "    seq_emd = embedding[0][:seq_len-1]\n",
    "    \n",
    "    return seq_emd\n",
    "\n",
    "\n",
    "def get_esm2_3B_features(sequence):\n",
    "  \n",
    "    # prepare input df in the format that model accepts\n",
    "    # data = [\n",
    "    #     (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    # ]\n",
    "    \n",
    "    # prepare dataframe of truncated string\n",
    "    data = [\n",
    "        (\"pid\", sequence),\n",
    "    ]\n",
    "    \n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter_esm(data)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = pretrained_model_esm(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "  \n",
    "    # return only residue level embeddings so that we can treat them exactly as prott5 features that we are already using\n",
    "    return token_representations[:,1:-1,:][0]\n",
    "\n",
    "\n",
    "\n",
    "# main function--------------------------------------------------------------------------------\n",
    "# load models\n",
    "prot_t5_model_ann = load_model(prot_t5_model_path)\n",
    "esm2_model_ann = load_model(esm2_model_path)\n",
    "ankh_model_ann = load_model(ankh_model_path)\n",
    "\n",
    "for seq_record in tqdm(SeqIO.parse(input_fasta_file, \"fasta\")):\n",
    "    prot_id = seq_record.id\n",
    "    sequence = str(seq_record.seq)\n",
    "    \n",
    "    # if sequence is longer than 1021, truncate\n",
    "    if len(sequence) > 1021:\n",
    "        sequence = get_1021_string(sequence)\n",
    "    \n",
    "    positive_predicted = []\n",
    "    negative_predicted = []\n",
    "    \n",
    "    # extract protT5 for full sequence and store in temporary dataframe \n",
    "    pt5_all = get_protT5_features(sequence)\n",
    "    ankh_all = get_ankh_features(sequence)\n",
    "    esm_all = get_esm2_3B_features(sequence)\n",
    "    \n",
    "    # generate embedding features and window for each amino acid in sequence\n",
    "    for index, amino_acid in enumerate(sequence):\n",
    "        \n",
    "        # check if AA is 'S' or 'T'\n",
    "        if amino_acid in ['S', 'T']:\n",
    "            \n",
    "            # we consider site one more than index, as index starts from 0\n",
    "            site = index + 1\n",
    "\n",
    "            # get ProtT5, ESM, ANN features extracted above\n",
    "            X_test_pt5 = pt5_all[index]\n",
    "            X_test_esm = esm_all[index]\n",
    "            X_test_ankh = ankh_all[index]\n",
    "            \n",
    "            # prediction results           \n",
    "            y_pred_ankh = ankh_model_ann.predict(np.array(X_test_ankh.reshape(1,1536)), verbose = 0)[0][0]\n",
    "            y_pred_esm = esm2_model_ann.predict(np.array(X_test_esm.reshape(1,2560)), verbose = 0)[0][0]\n",
    "            y_pred_prot_t5 = prot_t5_model_ann.predict(np.array(X_test_pt5.reshape(1,1024)), verbose = 0)[0][0]\n",
    "            \n",
    "            # combined result\n",
    "            voting_result = int(y_pred_ankh > cutoff_threshold_ankh) + int(y_pred_esm > cutoff_threshold_esm) + int(y_pred_prot_t5 > cutoff_threshold_prott5)\n",
    "            print(voting_result)\n",
    "            # append results to results_df\n",
    "            results_df.loc[len(results_df)] = [prot_id, site, amino_acid, y_pred_ankh, y_pred_prot_t5, y_pred_esm, int(voting_result > 1) ]\n",
    "\n",
    "# Export results \n",
    "print('Saving results ...')\n",
    "results_df.to_csv(output_csv_file, index = False)\n",
    "print('Results saved to ' + output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7c01d4-7be3-4882-83a4-d949991f7c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
